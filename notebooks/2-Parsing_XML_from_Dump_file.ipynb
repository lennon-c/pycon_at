{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9284d4d7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import lxml.etree as ET # to parse XML documents\n",
    "import pickle # to store the dictionary locally "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09c1cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Setting Up Paths Local Machine\n",
    "\n",
    "To follow along in this section:\n",
    "\n",
    "1. You will need to download and decompress the Wiktionary dump file.  \n",
    "    - You can download the latest version [here](https://dumps.wikimedia.org/dewiktionary/latest/dewiktionary-latest-pages-articles-multistream.xml.bz2) or refer to instructions for downloading specific versions [here](https://lennon-c.github.io/python-wikitext-parser-guide//Fetching%20XML%20data/Dump%20files/).\n",
    "2. Once you have done that, specify the path to the decompressed file in `XML_FILE`.\n",
    "3. By the end of this section, we will save our result as a dictionary and store it locally.  \n",
    "    - Therefore, do not forget to specify in which folder the dictionary should be saved in `DICT_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your own paths\n",
    "XML_FILE = Path(r'path\\to\\xml\\dewiktionary-20241020-pages-articles-multistream.xml')\n",
    "DICT_PATH = Path(r'path\\to\\dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56efb86",
   "metadata": {},
   "source": [
    "\n",
    "## Setting Up Paths in Google Colab \n",
    "\n",
    "- In Google Colab we are going to work with a toy version of the dump file that you can find in my github repo. \n",
    "- We will set `DICT_PATH` to the current working directory and set the file name in `XML_FILE` to \"playground_dump_20241020.xml\"  \n",
    "- The following code will download the xml file and save it as \"playground_dump_20241020.xml\" in the current working directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "XML_FILE = \"playground_dump_20241020.xml\"\n",
    "DICT_PATH = \"\"\n",
    "\n",
    "url =  \"https://raw.githubusercontent.com/lennon-c/pycon_at/refs/heads/main/data/playground_dump_20241020.xml\"\n",
    "\n",
    "urllib.request.urlretrieve(url, XML_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1b715",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Parsing the XML File\n",
    "Since we are working with a file, we cannot use the `ET.fromstring` function to parse the XML content. Instead, we must use the `ET.parse` function.\n",
    "\n",
    "Note that this process can take some time. On my computer, it takes approximately 42 seconds to load the entire XML tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ET.parse for a xml file\n",
    "tree = ET.parse(XML_FILE)\n",
    "print(type(tree)) # lxml.etree._ElementTree\n",
    "\n",
    "root = tree.getroot()\n",
    "print(type(root)) # <class 'lxml.etree._Element'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351367c8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The parser returns an `ElementTree` object. We use the `getroot()` method to access the root `Element`.\n",
    "\n",
    "\n",
    "## Displaying the XML Structure\n",
    "\n",
    "The XML structure of the dump file is quite large, so printing the entire tree would not only be inefficient but also quite overwhelming. To make it more manageable, let us modify our `print_tags_tree` function.\n",
    "\n",
    "We will add options to limit the number of children displayed for the root element and to control the depth of the tree.\n",
    "\n",
    "Here is our updated `print_tags_tree` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31219da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tags_tree(elem, level=0, only_tagnames=False, max_children=5, max_level=5):\n",
    "\n",
    "    tagname = ET.QName(elem).localname if only_tagnames else elem.tag\n",
    "    print(\" \" * 5 * level, level, tagname)\n",
    "\n",
    "    # Restrict depth\n",
    "    if level + 1 <= max_level:\n",
    "        for child_index, child in enumerate(elem):\n",
    "            print_tags_tree(child, level + 1, only_tagnames, max_children, max_level)\n",
    "            # Limit number of children of the root element\n",
    "            if level == 0 and child_index == max_children - 1:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269d04a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To display only the first 5 direct children of the root element and limit the tree to the first level:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tags_tree(root, only_tagnames=True, max_children=5, max_level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c3b6f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "To view the first 3 children of the root element and display two levels of the tree:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tags_tree(root, only_tagnames=True, max_children=3, max_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892b889",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "## Extracting Data\n",
    "\n",
    "### `element.findall` \n",
    "\n",
    "As with the previous section, we are interested in extracting the `page`, `title`, `ns`, and `text` tags.\n",
    "\n",
    "The main difference in structure here is that we now have multiple `page` elements, and we want to extract all of them.\n",
    "\n",
    "We cannot use `find`, because it will return only the first `page`. However, we can use the `findall` method instead, which will return a list of all `page` elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMESPACES = root.nsmap \n",
    "pages = root.findall('page', namespaces=NAMESPACES)\n",
    "print(len(pages)) # as of today 1281638"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ded335",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Notice that the latest dump file version contains more than one million pages, and retrieving them all takes approximately 45 seconds.\n",
    "\n",
    "Since retrieving all pages is time-consuming, we will store the relevant information locally in a dictionary and save it as a pickle file for quicker access in the future.\n",
    "\n",
    "We will create a dictionary, `dict_0`, using page titles as keys and their *wikitext* as values. Additionally, we will restrict the pages we store to those within the main Wiki namespace (`'0'`). We will discuss Wiki namespaces further when we parse *wikitext*.\n",
    "\n",
    "This process may take a couple of minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = '0'\n",
    "dict_0 = dict()\n",
    "for page in pages:\n",
    "    ns_elem = page.find('ns', namespaces=NAMESPACES)\n",
    "    if ns_elem.text == ns: \n",
    "        title = page.find('title', namespaces=NAMESPACES)\n",
    "        wikitext = page.find('revision/text', namespaces=NAMESPACES)\n",
    "        dict_0[title.text] = wikitext.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88767cf7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To check that our dictionary is correctly populated, let us print out part of the *wikitext* for a sample page:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e27cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_0['schön'][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ed087",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Saving the Dictionary Locally\n",
    "\n",
    "Once the dictionary is built, we save it locally using the `pickle` module, which allows us to store the dictionary in a serialized format. This way, we will not need to parse the XML file again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d941dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_file = DICT_PATH / f'wikidict_{ns}.pkl'\n",
    "        \n",
    "with open(dict_file, 'wb') as f:\n",
    "    pickle.dump(dict_0, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55425ecd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Loading Dictionary\n",
    "\n",
    "The next time you need to retrieve *wikitext*, simply load the dictionary from the pickle file and select the title page you need!\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "ns = '0'\n",
    "dict_file = DICT_PATH / f'wikidict_{ns}.pkl'\n",
    "\n",
    "with open(dict_file, 'rb') as f:\n",
    "    dict_0 = pickle.load(f)\n",
    "# 9 secs\n",
    "\n",
    "wikitext = dict_0['schön']\n",
    "print(wikitext[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf7d3c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "And we are done! Now we can retrieve any *wikitext* by the page title.  \n",
    "Next, we can cover how to parse *wikitext*."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
